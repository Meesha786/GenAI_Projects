{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit transformers accelerate sentencepiece pyngrok -q\n"
      ],
      "metadata": {
        "id": "Uo3FuT-p2M4l"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit ctransformers==0.2.27 langchain-core pyngrok huggingface_hub -q\n"
      ],
      "metadata": {
        "id": "LDbCvwymDs5_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Streamlit config\n",
        "st.set_page_config(page_title=\"Generate Emails\", page_icon=\"ðŸ“§\", layout=\"centered\")\n",
        "st.header(\"Generate Emails ðŸ“§\")\n",
        "\n",
        "# Sidebar model settings\n",
        "with st.sidebar:\n",
        "    st.subheader(\"Model Settings\")\n",
        "    gpu_layers = st.number_input(\"GPU layers (0 = CPU only)\", min_value=0, max_value=100, value=0, step=1)\n",
        "    max_new_tokens = st.slider(\"Max new tokens\", 64, 1024, 256, 32)\n",
        "    temperature = st.slider(\"Temperature\", 0.0, 1.5, 0.7, 0.05)\n",
        "\n",
        "@st.cache_resource(show_spinner=True)\n",
        "def load_llm(gpu_layers):\n",
        "    return AutoModelForCausalLM.from_pretrained(\n",
        "        \"TheBloke/Llama-2-7B-Chat-GGML\",             # repo name\n",
        "        model_file=\"llama-2-7b-chat.ggmlv3.q4_0.bin\", # file in repo\n",
        "        model_type=\"llama\",\n",
        "        gpu_layers=int(gpu_layers),\n",
        "        context_length=4096,\n",
        "        threads=os.cpu_count(),\n",
        "    )\n",
        "\n",
        "llm = load_llm(gpu_layers)\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are an expert assistant that writes clear, concise, and polite emails. \"\n",
        "    \"Keep the email well-structured, professional, and tailored to the requested style.\"\n",
        ")\n",
        "\n",
        "TEMPLATE = \"\"\"<s>[INST] <<SYS>>\n",
        "{system_prompt}\n",
        "<</SYS>>\n",
        "\n",
        "Write an email with {style} style that includes the topic: {email_topic}.\n",
        "\n",
        "Sender: {sender}\n",
        "Recipient: {recipient}\n",
        "\n",
        "Constraints:\n",
        "- Start with a greeting (e.g., Dear {recipient}).\n",
        "- Keep it concise and specific to the topic.\n",
        "- Close with a proper sign-off using the sender's name.\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt_tmpl = PromptTemplate(\n",
        "    input_variables=[\"system_prompt\", \"style\", \"email_topic\", \"sender\", \"recipient\"],\n",
        "    template=TEMPLATE,\n",
        ")\n",
        "\n",
        "def getLLMResponse(form_input, email_sender, email_recipient, email_style):\n",
        "    prompt = prompt_tmpl.format(\n",
        "        system_prompt=SYSTEM_PROMPT,\n",
        "        email_topic=form_input.strip(),\n",
        "        sender=email_sender.strip(),\n",
        "        recipient=email_recipient.strip(),\n",
        "        style=email_style.strip(),\n",
        "    )\n",
        "    text = llm(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        stop=[\"</s>\"]\n",
        "    )\n",
        "    return text.strip()\n",
        "\n",
        "# UI form\n",
        "form_input = st.text_area('Enter the email topic', height=200)\n",
        "col1, col2, col3 = st.columns([10, 10, 5])\n",
        "with col1:\n",
        "    email_sender = st.text_input('Sender Name')\n",
        "with col2:\n",
        "    email_recipient = st.text_input('Recipient Name')\n",
        "with col3:\n",
        "    email_style = st.selectbox('Writing Style', ('Formal', 'Appreciating', 'Not Satisfied', 'Neutral'))\n",
        "\n",
        "disabled = not (form_input and email_sender and email_recipient)\n",
        "\n",
        "if st.button(\"Generate\", disabled=disabled):\n",
        "    st.subheader(\"Generated Email\")\n",
        "    email_text = getLLMResponse(form_input, email_sender, email_recipient, email_style)\n",
        "    st.write(email_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITjOUwt0qlJV",
        "outputId": "20b0fa93-0a93-4c8f-94a1-26326372e054"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill ngrok\n"
      ],
      "metadata": {
        "id": "AmbvxdWZpoT7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import subprocess\n",
        "\n",
        "# Set your ngrok token\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve ngrok token securely\n",
        "NGROK_AUTH_TOKEN = userdata.get(\"NGROK_AUTH_TOKEN\")\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Start tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Access the app here: {public_url}\")\n",
        "\n",
        "# Run the app\n",
        "def run_app():\n",
        "    subprocess.run([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"])\n",
        "\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-08DxZu2qqU",
        "outputId": "64e73595-39ab-472e-83fa-ccdae2c5f8a1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access the app here: NgrokTunnel: \"https://a344ed1e17dc.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9hkv1eFc12T4"
      },
      "outputs": [],
      "source": []
    }
  ]
}